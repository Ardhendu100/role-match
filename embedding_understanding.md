Embeddings are used to convert text into vector representations that can be stored in a vector database and used for similarity search.
Embedding = Meaning in numbers.
Two similar texts will have similar vectors.
Example:
"Python backend developer"


"Backend engineer using Python"


These will produce vectors that are close in vector space.
Later we will measure that closeness using:
cosine similarity


That is how job matching works.

---------------------------------------------------------

ğŸ§  First: Why Do We Need Cosine Similarity?

When we create embeddings, we convert text into vectors:

"Python backend developer"
â†’ [0.21, -0.88, 0.43, ...]  (384 numbers)

Now the question is:

ğŸ‘‰ How do we measure if two vectors mean similar things?

We need a similarity function.

That function is:

Cosine similarity


ğŸ¯ What Cosine Similarity Measures

It measures:

The angle between two vectors.

Not the distance.
Not the magnitude.
Only the angle.

Why angle?

Because magnitude (vector length) doesnâ€™t represent meaning.
Direction represents meaning.

Think of vectors like arrows in space.

If two arrows point in:

Same direction â†’ Very similar â†’ cosine â‰ˆ 1

90Â° angle â†’ Unrelated â†’ cosine â‰ˆ 0

Opposite direction â†’ Opposite meaning â†’ cosine â‰ˆ -1

So result range is:

-1  â†’ completely opposite
 0  â†’ unrelated
+1  â†’ identical meaning


In embeddings, usually:

0.7+ â†’ very similar

0.4 â†’ somewhat related

0.1 â†’ unrelated

ğŸ¯ Why This Matters

This is EXACTLY how job matching works.

In our case:

Resume embedding
â†“
Compare against job embeddings
â†“
Sort by similarity
â†“
Top 5 matches

Thatâ€™s the core engine.

----------------------------------------------------

ğŸ§  Think Like an Engineer (Not Just a Rule Follower)

Cosine similarity range:

-1   â†’ opposite meaning
 0   â†’ unrelated
+1   â†’ identical meaning


So:

0.85 â†’ strong alignment

0.42 â†’ moderate alignment

0.05 â†’ almost unrelated

We choose the highest score because:

Higher cosine similarity = smaller angle between vectors = more semantic similarity.

---------------------------------------------------

ğŸ”¥ Now Big Realization

Your job matching system is basically:

For each job:
    score = cosine(resume, job)

Sort by score descending
Return top 5


Thatâ€™s it.

No magic.
No AI thinking.
Just vector math.

The LLM comes later to explain.

----------------------------------


ğŸ§  This Is Very Important

Embeddings are not truth.

They are:

The modelâ€™s learned representation of meaning.

So if two unrelated texts get 0.92 similarity, it means:

The model thinks they are similar

The training data shaped that understanding

The vector space structure caused them to align

It does NOT mean objective truth.


ğŸ”¥ This Is Why Model Choice Matters

Different embedding models produce:

Different vector spaces

Different similarity behavior

Different matching quality

Thatâ€™s why in real companies:

Model evaluation is important

You test retrieval quality

You tune thresholds


ğŸ§  Now You Are Thinking Like an AI Engineer

You just understood:

Matching = math

Meaning = learned representation

High similarity = model alignment, not absolute truth

This is foundational knowledge for:

RAG systems

Vector databases

AI search engines


---------------------------

ğŸ§  First Question

How are those numeric vectors created?

When you do:

model.encode("Python backend developer")


Internally this happens:

1ï¸âƒ£ Text â†’ Tokenization
2ï¸âƒ£ Tokens â†’ Numbers
3ï¸âƒ£ Numbers â†’ Neural Network
4ï¸âƒ£ Neural Network â†’ 384-dimensional vector

Letâ€™s unpack that.


Step 1 â€” Tokenization

The sentence:

"Python backend developer"


Gets split into tokens like:

["Python", "backend", "developer"]


Actually internally it becomes token IDs like:

[7592, 18723, 10234]


Each word becomes an integer.


Step 2 â€” Word Embeddings (Initial Layer)

Each token ID is mapped to a learned vector.

Example (simplified):

Python â†’ [0.21, -0.55, 0.90, ...]
backend â†’ [0.11, -0.22, 0.44, ...]

These are learned during training on massive datasets.


Step 3 â€” Transformer Layers

The model (MiniLM, which is based on BERT architecture) passes those vectors through multiple transformer layers.

Inside each layer:

Attention mechanism

Matrix multiplications

Nonlinear transformations

Context mixing

After many layers, the model produces a final contextual representation.

That final representation becomes your 384-length embedding.



ğŸ¯ Now Your Main Question

How can two unrelated texts get 0.92 similarity?

There are a few possibilities.

1ï¸âƒ£ The Model Sees Hidden Similarity

Example:

Text A:

"I work with neural networks"


Text B:

"I build AI systems"


You may think different.
Model thinks related.

So similarity high.

2ï¸âƒ£ Dataset Bias

The model learned relationships from its training data.

If two concepts frequently appeared together in training,
their vectors may become aligned.

3ï¸âƒ£ Embedding Space Compression

Remember:

We compress meaning into only 384 numbers.

That is a HUGE compression.

Language is infinitely complex.
Model must approximate.

So sometimes:
Two unrelated texts may accidentally land in nearby space.

This is called:

False positive in semantic similarity.


4ï¸âƒ£ Limited Model Capacity

MiniLM is small.

Smaller models:

Faster

Cheaper

But less precise

Bigger models usually produce cleaner embedding spaces.

-------------------------------------------------------------------

ğŸ”¥ Deep Engineering Truth

Vector similarity is:

Approximate semantic similarity.

Not perfect.
Not logical.
Not reasoning-based.

It is learned geometry.